{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqV8orEjKpnX",
        "outputId": "384ae07d-2307-4916-ae36-334cf0f97847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.739777 M parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.3213, val loss 4.3130\n",
            "step 500: train loss 1.9163, val loss 2.0195\n",
            "step 1000: train loss 1.6001, val loss 1.7811\n",
            "step 1500: train loss 1.4684, val loss 1.6593\n",
            "step 2000: train loss 1.3887, val loss 1.5967\n",
            "step 2500: train loss 1.3467, val loss 1.5801\n",
            "step 3000: train loss 1.3074, val loss 1.5409\n",
            "step 3500: train loss 1.2761, val loss 1.5302\n",
            "step 4000: train loss 1.2418, val loss 1.5189\n",
            "step 4500: train loss 1.2208, val loss 1.5124\n",
            "step 4999: train loss 1.2033, val loss 1.5146\n",
            "\n",
            "Most Lord Warwick, wiltune child him to him\n",
            "Thought her sweet bound himself at to a MostleRy.\n",
            "All is he\n",
            "That thus turn thus, the well occk as; fear whose horse,\n",
            "seen gave the time at Plain.\n",
            "'Tis utchiary service, as sto it intends\n",
            "Can enduuing his bloody; father doth each ended,\n",
            "The revery with in noble set men twelves by his:\n",
            "And he now the welvily will not bear a thoot;\n",
            "Where he, and make concudy, what is Rome to run the know?\n",
            "If have found one enemy and go a determity.\n",
            "\n",
            "JULIET:\n",
            "This is tongue\n",
            "Coherence Test - Generated text: Once upon a time, there was a brave knight named of the door:\n",
            "Beath the prince so doth through thy heart-highness,\n",
            "The phress treason, great was the\n",
            "Contextual Relevance Test - Generated text: Romeo stood beneath Juliet's balcony and whispered,\n",
            "To much the world of the quickly have remember'd,\n",
            "And tell myself when I'll to meet me so die.\n",
            "And \n",
            "Stylistic Alignment Test - Generated text: To be or not to be, that is the struck and will present.\n",
            "\n",
            "BUCKINGHAM:\n",
            "My lord, then they are thee to death that lamb,\n",
            "And be af\n",
            "Complexity Test - Generated text: In the darkest depths of the enchanted forest, these blows\n",
            "pluck of a gozent drums! make death the time one\n",
            "In his priviles, image that he was?\n",
            "It is \n",
            "Cultural Reference Test - Generated text: Hark! What light through yonder window the husband?\n",
            "\n",
            "Second Servingman:\n",
            "Where he had soldiers becomes ureled.\n",
            "\n",
            "ESCALUS:\n",
            "Now honours not le\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32  # how many independent sequences will we process in parallel?\n",
        "block_size = 128  # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "#wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, stoi, itos):\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "\n",
        "    def encode(self, text):\n",
        "        return torch.tensor([self.stoi.get(c, self.stoi[' ']) for c in text], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join(self.itos[token] for token in tokens if token in self.itos)\n",
        "\n",
        "# Example character set mappings, replace these with your actual mappings\n",
        "with open(\"input.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))  # Define 'text' variable as your dataset text content\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "tokenizer = SimpleTokenizer(stoi, itos)\n",
        "\n",
        "def generate_text(prompt, model, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt).to(device)\n",
        "        generated_tokens = model.generate(tokens, max_new_tokens=max_new_tokens)\n",
        "        generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
        "    return generated_text\n",
        "\n",
        "# Example of generating text for different types of prompts\n",
        "def run_text_generation_tests(model):\n",
        "    test_prompts = {\n",
        "        \"Coherence Test\": \"Once upon a time, there was a brave knight named\",\n",
        "        \"Contextual Relevance Test\": \"Romeo stood beneath Juliet's balcony and whispered,\",\n",
        "        \"Stylistic Alignment Test\": \"To be or not to be, that is\",\n",
        "        \"Complexity Test\": \"In the darkest depths of the enchanted forest, the\",\n",
        "        \"Cultural Reference Test\": \"Hark! What light through yonder window\"\n",
        "    }\n",
        "\n",
        "    for test_name, prompt in test_prompts.items():\n",
        "        print(f\"{test_name} - Generated text:\", generate_text(prompt, model, max_new_tokens=100))\n",
        "\n",
        "# Assuming 'model' is your trained GPTLanguageModel instance\n",
        "run_text_generation_tests(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KvXZ4yg-QkLW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_epochs = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iterations = 200\n",
        "embedding_dim = 384\n",
        "num_heads = 6\n",
        "num_layers = 6\n",
        "dropout_rate = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download and load the dataset\n",
        "#dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "#!wget {dataset_url}\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Preprocess the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
        "index_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [char_to_index[c] for c in s]\n",
        "decode = lambda l: ''.join([index_to_char[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "train_data_len = int(0.9 * len(data))\n",
        "train_data = data[:train_data_len]\n",
        "val_data = data[train_data_len:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    indices = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    input_sequences = torch.stack([data[i:i + block_size] for i in indices])\n",
        "    target_sequences = torch.stack([data[i + 1:i + block_size + 1] for i in indices])\n",
        "    input_sequences, target_sequences = input_sequences.to(device), target_sequences.to(device)\n",
        "    return input_sequences, target_sequences\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    train_loss = torch.zeros(eval_iterations)\n",
        "    val_loss = torch.zeros(eval_iterations)\n",
        "    for k in range(eval_iterations):\n",
        "        X, Y = get_batch('train')\n",
        "        logits, loss = model(X, Y)\n",
        "        train_loss[k] = loss.item()\n",
        "        X, Y = get_batch('val')\n",
        "        logits, loss = model(X, Y)\n",
        "        val_loss[k] = loss.item()\n",
        "    model.train()\n",
        "    return train_loss.mean(), val_loss.mean()\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        self.query_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.attention_mask = torch.tril(torch.ones(block_size, block_size)).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        query = self.query_proj(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = self.key_proj(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = self.value_proj(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / self.head_dim ** 0.5\n",
        "        attention_scores = attention_scores.masked_fill(self.attention_mask[:x.size(1), :x.size(1)] == 0, float('-inf'))\n",
        "        attention_probs = self.dropout(F.softmax(attention_scores, dim=-1))\n",
        "\n",
        "        out = torch.matmul(attention_probs, value).transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_dim)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(FeedForwardLayer, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, 4 * embedding_dim)\n",
        "        self.fc2 = nn.Linear(4 * embedding_dim, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.feed_forward = FeedForwardLayer(embedding_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x) + residual\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        residual = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x) + residual\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.position_embeddings = nn.Embedding(block_size, embedding_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(embedding_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(torch.arange(seq_len, device=device))\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Generated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generate_text(prompt, model, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Assuming 'model' is your trained GPTLanguageModel instance\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m run_text_generation_tests(\u001b[43mmodel\u001b[49m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, stoi, itos):\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "\n",
        "    def encode(self, text):\n",
        "        return torch.tensor([self.stoi.get(c, self.stoi[' ']) for c in text], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join(self.itos[token] for token in tokens if token in self.itos)\n",
        "\n",
        "# Example character set mappings, replace these with your actual mappings\n",
        "with open(\"input.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))  # Define 'text' variable as your dataset text content\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "tokenizer = SimpleTokenizer(stoi, itos)\n",
        "\n",
        "def generate_text(prompt, model, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = tokenizer.encode(prompt).to(device)\n",
        "        generated_tokens = model.generate(tokens, max_new_tokens=max_new_tokens)\n",
        "        generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
        "    return generated_text\n",
        "\n",
        "# Example of generating text for different types of prompts\n",
        "def run_text_generation_tests(model):\n",
        "    test_prompts = {\n",
        "        \"Coherence Test\": \"Once upon a time, there was a brave knight named\",\n",
        "        \"Contextual Relevance Test\": \"Romeo stood beneath Juliet's balcony and whispered,\",\n",
        "        \"Stylistic Alignment Test\": \"To be or not to be, that is\",\n",
        "        \"Complexity Test\": \"In the darkest depths of the enchanted forest, the\",\n",
        "        \"Cultural Reference Test\": \"Hark! What light through yonder window\"\n",
        "    }\n",
        "\n",
        "    for test_name, prompt in test_prompts.items():\n",
        "        print(f\"{test_name} - Generated text:\", generate_text(prompt, model, max_new_tokens=100))\n",
        "\n",
        "# Assuming 'model' is your trained GPTLanguageModel instance\n",
        "run_text_generation_tests(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
